{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert\n",
    "\n",
    "\n",
    "#_____________________________________\n",
    "\n",
    "def angle(pos,i,dimension):\n",
    "    return pos / np.power(10000, (2 * (i//2)) / np.float32(dimension))\n",
    "   \n",
    "def positional(pos,dimension):\n",
    "    posmatrix=np.arange(pos)[:,np.newaxis]\n",
    "    dimmatrix=np.arange(dimension)[np.newaxis,:]\n",
    "    angles=angle(posmatrix,dimmatrix,dimension)\n",
    "    i=0\n",
    "    for index1,substack in enumerate(angles):\n",
    "        i=0\n",
    "        for index2,value in enumerate(substack):\n",
    "            if i%2==0:\n",
    "                substack[index2]=np.sin(value)#~we apply what is desired\n",
    "            else:\n",
    "                substack[index2]=np.cos(value)\n",
    "            i+=1\n",
    "    \n",
    "    angles=angles[np.newaxis,...]\n",
    "    #this will return a [ [](dimensions) ,... []] for every position we have dimension values think it as an embedding dimension\n",
    "    return tf.cast(angles,dtype=tf.float32)\n",
    "#And we have or positional embedding fully defined\n",
    "\n",
    "def paddingmask(data):\n",
    "    #data will have dims (batchsize, sequence)\n",
    "    #for attention layers we will include two extra dimensions(batchsize,1,1,sequence)\n",
    "    data = tf.cast(tf.math.equal(data, 0), tf.float32)\n",
    "    #we do math equal in case of true cast will make it a 1.0 otherwise it will be false and cast into 0.0\n",
    "    \n",
    "    return data[:,tf.newaxis,tf.newaxis,:]\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len\n",
    "\n",
    "def lookahead(dimension): #Dimensions has shape (dim,dim)\n",
    "    #what we want is something that has first 0.0 and 1.0 for all values in the first row and then add another 0.0 for each row until \n",
    "    #completed\n",
    "    #we will give dimension of the maximum sequence length ie(1,5) a sequence of five words \n",
    "    #in general we will have a (sequence,sequence) output dimension\n",
    "    res=np.ones(dimension)\n",
    "    for index,value in enumerate(res):\n",
    "        res[index:,index]=0.0\n",
    "        #we set from (x,x) to below as 0\n",
    "        \n",
    "    return tf.convert_to_tensor(res)\n",
    "\n",
    "\n",
    "\n",
    "#This one is the one created by tensorflow post, it will come in handy the concept is exactly the same    \n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def dotproductattention(q,k,v,mask):\n",
    "    \n",
    "    \n",
    "    #First we need to matmul q,k transposing k\n",
    "    resmatmul=tf.matmul(q,k,transpose_b=True)\n",
    "    \n",
    "    #then we need the dimension of k d_k \n",
    "    kdim=float(tf.shape(k)[-1])\n",
    "    resmatmul=resmatmul/(tf.math.sqrt(kdim))\n",
    "    \n",
    "    #It may be useful to include the situation where mask is None\n",
    "    if mask is not None:\n",
    "        resmatmul += mask*-1e9\n",
    "    \n",
    "    ressoftmax=tf.nn.softmax(resmatmul,axis=-1) # we need to do softmax on the last dimension\n",
    "    return tf.matmul(ressoftmax,v), ressoftmax\n",
    "    \n",
    "    \n",
    "class MultiAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,modeldim,layersnum): \n",
    "        # we need to inherit all of the keras information of th super\n",
    "        super().__init__()\n",
    "        #now we can start working\n",
    "        \n",
    "        #WEIGHTS FOR Q,K,V\n",
    "        self.wq=tf.keras.layers.Dense(modeldim)\n",
    "        self.wk=tf.keras.layers.Dense(modeldim)\n",
    "        self.wv=tf.keras.layers.Dense(modeldim)\n",
    "                          \n",
    "            \n",
    "        #WEIGHTS FO AFTER CONCATENTION\n",
    "        self.dense=tf.keras.layers.Dense(modeldim)\n",
    "        \n",
    "        #extra information\n",
    "        \n",
    "        self.modeldim=modeldim\n",
    "        self.layersnum=layersnum\n",
    "        assert modeldim % self.layersnum == 0\n",
    "        self.depth=modeldim//self.layersnum\n",
    "        \n",
    "        \n",
    "    #WE NEED TO SPLIT INTO modeldim /layresnum\n",
    "    \n",
    "    #All this part is heavily optimized thanks to the tensorflow tutorial for transformers\n",
    "    #The main idea is to split the last dimension into (number of layers, model dimension)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        #Split the last dimension into (layersnum, depth).\n",
    "        #Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        #\n",
    "        x = tf.reshape(x, (batch_size, -1, self.layersnum, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, layersnum, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, layersnum, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, layersnum, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, layersnum, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, layersnum, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = dotproductattention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.modeldim))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "          tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "      ])\n",
    "\n",
    "\n",
    "#here we have the definition of the layers\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        #we can see that q,k,v is the same vector given for encoders\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model) we do an attention\n",
    "        attn_output = self.dropout1(attn_output, training=training)#droupout for better training\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model) normalization\n",
    " \n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model) feedforwards\n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model) \n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "\n",
    "#AND finally we define the encoder and the decoder\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                   maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model) we create the embedding so it is of the dimensions we want\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "    \n",
    "    \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                   target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                 input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "    # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = paddingmask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = paddingmask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar) #lookaheadmask\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
