{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic importations\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is mainly for deep understanding of paper Attention is all you need \n",
    "## Check it out here : https://arxiv.org/abs/1706.03762 .  \n",
    "Basically is creating a transformer to be able to translate words from english to portuguese in this specific case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\MI-CUENTA\\tensorflow_datasets\\ted_hrlr_translate\\es_to_pt\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87ad0c3ca604670a39077e377394734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67cecd8be294d958327fd4e4df12573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f82ffb4416447d943d3e59c009b830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generating splits...', max=3.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating train examples...', max=1.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\\\Users\\\\MI-CUENTA\\\\tensorflow_datasets\\\\ted_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating validation examples...', max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\\\Users\\\\MI-CUENTA\\\\tensorflow_datasets\\\\ted_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Generating test examples...', max=1.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Shuffling C:\\\\Users\\\\MI-CUENTA\\\\tensorflow_datasets\\\\ted_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset ted_hrlr_translate downloaded and prepared to C:\\Users\\MI-CUENTA\\tensorflow_datasets\\ted_hrlr_translate\\es_to_pt\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#We use the data provided by Ted fundation in tensorflow_datasets\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/es_to_pt', with_info=True,\n",
    "                               as_supervised=True)\n",
    "\n",
    "\n",
    "\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will call data a class to not have a random global variable \n",
    "class Data():\n",
    "    def __init__(self):\n",
    "        #alternitavely we can ask in innit what language we want to train\n",
    "        i=0\n",
    "        #WE WILL GET THE DATASET IN WORKING CONDITIONS EVERY LIST(TRAIN_EXMAPLES[I]) GIVES US A (EXAMPLE 1, EXAMPLE 2)\n",
    "        pt=[]\n",
    "        es=[]\n",
    "        #WE NEED TO DECODE IT TO UTF-8 SO IT IS PRESENTABLE AND WE ONLY WANT THE NUMPY\n",
    "        for es_example, pt_example in list(train_examples):\n",
    "            pt.append((pt_example.numpy()).decode('utf-8'))\n",
    "            es.append((es_example.numpy()).decode('utf-8'))\n",
    "        self.pt=pt\n",
    "        self.es=es\n",
    "        #And with this we have or data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to think about input embedding .\n",
    "Because obviously computers dont understand strings, we need some kind of input. We will build an embedding from zero\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset -> will help us generate a word for word construction\n",
    "But for this case we need it to be object database so Data() is not useful in this case, although we will use it later\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos=Data()\n",
    "# in this case lets generate a vocabulary of 5000 words\n",
    "# WE USE MAP AND LAMBDA MAP MAPS ONTO ARRAY AND SIMPLY LAMBDA ES,PT: ES IS SETTING FROM BOTH ES AND PT SETTING THE RESULT TO ES\n",
    "# THE RESEVED TOKENS ARE SIMPLY START FINISH AND CONDITRIONS\n",
    "es_vocab = bert.bert_vocab_from_dataset(train_examples.map(lambda es,pt:es),vocab_size =5000,reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"])\n",
    "pt_vocab = bert.bert_vocab_from_dataset(train_examples.map(lambda es,pt:pt),vocab_size =5000,reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w',encoding='utf-8') as f:\n",
    "        for token in vocab:\n",
    "              print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('es_vocab.txt',es_vocab)\n",
    "write_vocab_file('pt_vocab.txt',pt_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this we have already created a vocabulary. To  encapsulate everything we could put it in a class in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabgen:\n",
    "    def __init__(self,filename1,filename2,num):\n",
    "        es_vocab = bert.bert_vocab_from_dataset(train_examples.map(lambda es,pt:es),vocab_size =num,reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"])\n",
    "        pt_vocab = bert.bert_vocab_from_dataset(train_examples.map(lambda es,pt:pt),vocab_size =num,reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"])\n",
    "        #WE CREATE THE VOCABS AND WE WRITE THEM\n",
    "        write_vocab_file(filename1,es_vocab)\n",
    "        write_vocab_file(filename2,pt_vocab)\n",
    "        #We could make it even better so that given two languages we create the vocabs but is not necessary at the moment\n",
    "        \n",
    "    def write_vocab_file(filepath, vocab):\n",
    "        with open(filepath, 'w',encoding='utf-8') as f:\n",
    "            for token in vocab:\n",
    "                  print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocabgen at 0x20eee2bef40>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocabgen('es_vocab.txt','pt_vocab.txt',5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
