{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afcb3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run DataandTokenizer.ipynb\n",
    "%run Transformer.ipynb\n",
    "import time\n",
    "#This are the parameters that define our transformers\n",
    "layersnum = 4\n",
    "modeldim = 128\n",
    "dff = 512 #<- this is for the RELu layers between layers\n",
    "num_heads = 4 \n",
    "dropout_rate = 0.1\n",
    "\n",
    "\n",
    "class Translator(tf.Module):\n",
    "    def __init__(self, embed, transformer):\n",
    "        self.embed = embed\n",
    "        self.transformer = transformer\n",
    "        checkpoint_path = \"./build\"\n",
    "\n",
    "        ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                               optimizer=optimizer)\n",
    "\n",
    "        ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    \n",
    "        ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "        ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    def __call__(self, sentence, max_length=75):\n",
    "    # input sentence is portuguese, hence adding the start and end token\n",
    "        data=[]\n",
    "        dataes=[]\n",
    "        datapt=[]\n",
    "        dataes.append(sentence)\n",
    "        data.append(dataes)\n",
    "        data.append(datapt)\n",
    "        sentence=self.embed.tokenize(data)\n",
    "        encoder_input = sentence[0]\n",
    "        data2=[]\n",
    "        data2.append(encoder_input)\n",
    "        data2.append([[2]])\n",
    "        # as the target is english, the first token to the transformer should be the\n",
    "        # english start token.\n",
    "        start = sentence[0][0][0][tf.newaxis]#WE USE THE IDS IN spanish BECAUSE THEY ARE THE SAME IN PT\n",
    "        end = sentence[0][0][-1][tf.newaxis] #there could be some padding!\n",
    "        # `tf.TensorArray` is required here (instead of a python list) so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions, _ = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "          # select the last token from the seq_len dimension\n",
    "            predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "          # concatentate the predicted_id to the output which is given to the decoder\n",
    "          # as its input.\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "    # output.shape (1, tokens)\n",
    "        data=[]\n",
    "        data.append([[2]])\n",
    "        data.append(output[tf.newaxis][0])\n",
    "        text = (self.embed.detokenize(data)[1].numpy())[0]\n",
    "        text=text.decode('UTF-8')\n",
    "        text=text.removeprefix('[START]')\n",
    "        \n",
    "        text=text.removesuffix('[END]')\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop. So recalculate them outside\n",
    "        # the loop.\n",
    "        _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "\n",
    "        return text, attention_weights\n",
    "\n",
    "        \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = modeldim\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "learning_rate = CustomSchedule(modeldim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b3c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be18808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
